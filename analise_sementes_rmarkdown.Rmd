---
title: "Análise e classificação de sementes de abóbora"
author: "Marcos Perazo Viana"
date: '`r format(Sys.Date(), "%d de %B de %Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(plotly)
library(ggrepel)
library(knitr)
library(kableExtra) 
library(reshape2)
library(PerformanceAnalytics)
library(psych)
library(ltm)
library(Hmisc)
library(misc3d)
library(plot3D) 
library(cluster)
library(factoextra)
library(clusterCrit)
library(mclust)
library(clusterSim)
library(aricode)
library(fossil)
library(dendextend)
library(fpc)
library(NbClust)
library(ggplot2)
library(gridExtra)
library(tidyverse)
library(plotly)
library(ClusterR)
library(reshape2)
library(GGally)
library(readr)
library(sf)
library(ade4)
library(readxl)
library(dplyr)
library(rmarkdown)
library(tinytex)
library(viridis)
```

## Ambiente do RStudio

![Ambiente RStudio](
C:\Users\peraz\OneDrive\Arquivo\Marcos\Curso_de_Data_Science\Segmentacao\Visualizacao_e_Relatorios_de_Segmento\Projeto\imagens\RStudio.png)

## A base escolhida

A base de dados escolhida se encontra no endereço [Pumpkin Seeds Dataset](https://www.kaggle.com/datasets/muratkokludataset/pumpkin-seeds-dataset/data). Ela foi criada para classificação de sementes com base em características morfológicas extraídas de imagens. Isso tem aplicações diretas em: Agricultura de precisão, estudos botânicos e em processos industriais de seleção de sementes.

```{r Carregamento da base de dados}
dados_originais <- read_excel("Pumpkin_Seeds_Dataset.xlsx")
```

```{r Reduzindo a quantidade de registros}
set.seed(223)
dados_reduzidos <- dados_originais %>% slice_sample(n = 500)
dados_reduzidos <- as.data.frame(dados_reduzidos)
```

```{r Boxplot}
dados_long <- melt(dados_reduzidos, id.vars = "Class")

ggplot(dados_long, aes(x=Class, y=value, fill=Class)) +
  geom_boxplot(alpha=0.7) +
  facet_wrap(~ variable, scales='free_y') +
  labs(title = "Boxplots dos atributos por classe")
```

```{r Gráfico com a correlação entre cada um dos pares de variáveis possíveis}
ggpairs(dados_reduzidos, 
        columns = 1:12,      # Apenas variáveis numéricas
        aes(color = Class),  # Cor por classe
        upper = list(continuous = "points"), 
        lower = list(continuous = "smooth"), 
        diag = list(continuous = "densityDiag")) +
  theme_linedraw()
```

```{r Exclusão da coluna Class}
dados <- dados_reduzidos %>% dplyr::select(-Class)
```

\tiny
```{r Análise inicial}
knitr::kable(head(dados), caption = "Primeiras linhas do conjunto de dados de sementes de abóbora")
str(dados)         # Estrutura das variáveis
summary(dados)     # Estatísticas básicas
```
\normalsize

O data frame dados possui 16 variáveis numéricas que descrevem características morfológicas das sementes de abóbora, como área, perímetro, redondeza e razão de aspecto. Essas variáveis serão utilizadas para classificar os tipos de sementes presentes no conjunto.

```{r Normalização dos dados}
dados_padronizados <- scale(dados)
```

```{r Obtendo os dados reais}
ground_truth <- dados_reduzidos$Class
n_rows <- nrow(dados_padronizados)
k_clusters <- 2 # numero de clusters
```

## K-MEANS
```{r Execução do K-Means}
set.seed(223)
kmeans_res <- kmeans(dados_padronizados, centers = k_clusters, nstart = 25)
```

## PAM
```{r Execução do PAM}
pam_res <- pam(dados_padronizados, k = k_clusters)
```

## HIERÁRQUICO
```{r Execução do modelo hierarquico}
dist_matrix <- dist(dados_padronizados, method = "euclidean")
hc_res <- hclust(dist_matrix, method = "ward.D2") # linkage Ward
hc_clusters <- cutree(hc_res, k = k_clusters)
```

## MÉTRICAS INTERNAS

### Silhueta
```{r Silhueta}
sil_kmeans <- silhouette(kmeans_res$cluster, dist(dados_padronizados))
sil_pam <- silhouette(pam_res$clustering, dist(dados_padronizados))
sil_hc <- silhouette(hc_clusters, dist(dados_padronizados))
```

### WSS (Within-cluster Sum of Squares)
```{r WSS}
wss_kmeans <- kmeans_res$tot.withinss
wss_pam <- sum(sapply(1:k_clusters, function(k) sum((dados_padronizados[pam_res$clustering == k, ] - pam_res$medoids[k, ])^2)))
wss_hc <- sum(sapply(1:k_clusters, function(k) {
  cluster_points <- dados_padronizados[hc_clusters == k, , drop = FALSE]
  center <- colMeans(cluster_points)
  sum(rowSums((cluster_points - center)^2))
}))
```

### Davies-Bouldin
```{r Davies-Bouldin}
db_kmeans <- index.DB(dados_padronizados, kmeans_res$cluster)$DB
db_pam <- index.DB(dados_padronizados, pam_res$clustering)$DB
db_hc <- index.DB(dados_padronizados, hc_clusters)$DB
```

### RESULTADOS
```{r Resultados}
cat("Resultados (Métricas Internas):\n")
cat("K-Means - Silhueta:", mean(sil_kmeans[, 3]),
    " WSS:", wss_kmeans,
    " DBI:", db_kmeans, "\n")

cat("PAM     - Silhueta:", mean(sil_pam[, 3]),
    " WSS:", wss_pam,
    " DBI:", db_pam, "\n")

cat("Hierárq - Silhueta:", mean(sil_hc[, 3]),
    " WSS:", wss_hc,
    " DBI:", db_hc, "\n")

resultados <- data.frame(
  Algoritmo = c("K-Means", "PAM", "Hierárquico"),
  Silhueta = c(mean(sil_kmeans[, 3]), mean(sil_pam[, 3]), mean(sil_hc[, 3])),
  WSS = c(wss_kmeans, wss_pam, wss_hc),
  DBI = c(db_kmeans, db_pam, db_hc)
)

print(resultados)
```

### Criação do Gráfico com os resultados
```{r Gráfico Silhueta}
p1 <- ggplot(resultados, aes(x = Algoritmo, y = Silhueta, fill = Algoritmo)) +
  geom_bar(stat = "identity") +
  ggtitle("Comparação - Coeficiente de Silhueta") +
  theme_minimal()
```

```{r Gráfico WSS}
p2 <- ggplot(resultados, aes(x = Algoritmo, y = WSS, fill = Algoritmo)) +
  geom_bar(stat = "identity") +
  ggtitle("Comparação - WSS (Within-cluster Sum of Squares)") +
  theme_minimal()
```

```{r Gráfico DBI}
p3 <- ggplot(resultados, aes(x = Algoritmo, y = DBI, fill = Algoritmo)) +
  geom_bar(stat = "identity") +
  ggtitle("Comparação - Davies-Bouldin Index") +
  theme_minimal()
```

```{r Organizar em grid 1x3}
grid.arrange(p1, p2, p3, nrow = 3)
```

Nesse cenário, o algoritmo PAM apresentou o melhor desempenho geral, com o maior índice de silhueta e o menor DBI, indicando boa separação entre os grupos e baixa sobreposição. Apesar do WSS elevado, o modelo mostrou-se mais coeso e interpretável.

## Aplicação do algorítmo PCA nos dados normalizados

```{r  Coeficientes de correlação de Pearson para cada par de variáveis}
rho <- rcorr(as.matrix(dados_padronizados), type="pearson")
corr_coef <- rho$r          # Matriz de correlações
corr_sig <- round(rho$P, 5) # Matriz com p-valor dos coeficientes
```

```{r Elaboração de um mapa de calor das correlações de Pearson entre as variáveis}
# Calcula correlação e transforma em formato longo
correlacoes <- cor(dados_padronizados)
cor_melt <- melt(correlacoes)

# Renomeia coluna de correlação
names(cor_melt) <- c("Var1", "Var2", "Correlacao")

# Mapa de calor com ggplot2
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = Correlacao)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "B", direction = 1) +
  labs(title = "Mapa de calor das correlações de Pearson",
       x = NULL, y = NULL, fill = "Correlação") +
  theme_bw(base_size = 10) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r  Teste de esfericidade de Bartlett}
cortest.bartlett(dados_padronizados)
```

Encontramos p.value é igual a zero

```{r Elaboração da análise fatorial por componentes principais}
fatorial <- principal(dados_padronizados,
                      nfactors = ncol(dados_padronizados),
                      rotate = "none",
                      scores = TRUE)
cum_var <- fatorial$Vaccounted["Cumulative Var", ]
cum_var
```

```{r Encontra o menor número de componentes que atinge ou ultrapassa 70%}
num_comp <- which(cum_var >= 0.7)[1]
num_comp
```


```{r Variância compartilhada pelas variáveis originais para a formação de cada fator}
variancia_compartilhada <- as.data.frame(fatorial$Vaccounted) %>% 
  slice(1:3)

rownames(variancia_compartilhada) <- c("Autovalores",
                                       "Prop. da Variância",
                                       "Prop. da Variância Acumulada")

round(variancia_compartilhada, 3) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", 
                full_width = FALSE, 
                font_size = 20)
```


```{r Obtenção do dataframe com o resultado do PCA}
pca_df <- data.frame(
  fatorial$scores[, 1:2], 
  classe_da_semente = dados_reduzidos$Class
)
```

```{r Exclusão coluna classe_da_semente}
pca_df_dados <- pca_df %>% dplyr::select(-classe_da_semente)
```


```{r Método da silhueta para os dados após PCA}
fviz_nbclust(pca_df_dados, kmeans, method = "silhouette") +
  ggplot2::labs(title = "Silhouette Method")
```

O número de centróides pode ser obtido utilizando o índice de Silhueta. No gráfico acima, podemos verificar que são determinados nove centróides. Porém esse valor não condiz com os dados reais, que são apenas dois tipos de sementes diferentes.

```{r Distribuição dos dados após o PCA}
ggplot(pca_df, aes(x = PC1, y = PC2, color = classe_da_semente)) +
  geom_point(size = 3, alpha = 0.8) +
  labs(title = "PCA - Classe da Semente",
       x = "Componente Principal 1",
       y = "Componente Principal 2") +
  theme_minimal()
```

## Análise dos dados após PCA

```{r Boxplot após PCA}
pca_df_long <- melt(pca_df, id.vars = "classe_da_semente")

ggplot(pca_df_long, aes(x=classe_da_semente, y=value, fill=classe_da_semente)) +
  geom_boxplot(alpha=0.7) +
  facet_wrap(~ variable, scales='free_y') +
  labs(title = "Boxplots dos componentes gerados no PCA")
```

```{r Exclusão da coluna classe_da_semente}
dados_pca_df <- pca_df %>% dplyr::select(-classe_da_semente)
```

```{r Análise inicial apos PCA}
knitr::kable(head(pca_df), caption = "Primeiras linhas do conjunto de dados de sementes de abóbora após o PCA")
str(dados_pca_df)         # Estrutura das variáveis
summary(dados_pca_df)     # Estatísticas básicas
```

```{r Obtenção dos dados reais após PCA}
ground_truth_pca_df <- pca_df$classe_da_semente
n_rows_pca_df <- nrow(dados_pca_df)
k_clusters_pca_df <- 2 # numero de clusters
```

## K-MEANS
```{r Execução do K-Means após PCA}
set.seed(223)
kmeans_res_pca_df <- kmeans(dados_pca_df, centers = k_clusters, nstart = 25)
```

## PAM
```{r Execução do PAM após PCA}
pam_res_pca_df <- pam(dados_pca_df, k = k_clusters)
```

## HIERÁRQUICO
```{r Execução do Hierárquico após PCA}
dist_matrix_pca_df <- dist(dados_pca_df, method = "euclidean")
hc_res_pca_df <- hclust(dist_matrix_pca_df, method = "ward.D2") # linkage Ward
hc_clusters_pca_df <- cutree(hc_res_pca_df, k = k_clusters)
```

## MÉTRICAS INTERNAS

### Silhueta
```{r Silhueta após PCA}
sil_kmeans_pca_df <- silhouette(kmeans_res_pca_df$cluster, dist(dados_pca_df))
sil_pam_pca_df <- silhouette(pam_res_pca_df$clustering, dist(dados_pca_df))
sil_hc_pca_df <- silhouette(hc_clusters_pca_df, dist(dados_pca_df))
```

### WSS (Within-cluster Sum of Squares)
```{r WSS após PCA}
wss_kmeans_pca_df <- kmeans_res_pca_df$tot.withinss
wss_pam_pca_df <- sum(sapply(1:k_clusters_pca_df, function(k) sum((dados_pca_df[pam_res_pca_df$clustering == k, ] - pam_res_pca_df$medoids[k, ])^2)))
wss_hc_pca_df <- sum(sapply(1:k_clusters_pca_df, function(k) {
  cluster_points <- dados_pca_df[hc_clusters == k, , drop = FALSE]
  center <- colMeans(cluster_points)
  sum(rowSums((cluster_points - center)^2))
}))
```

### Davies-Bouldin
```{r Davies-Bouldin após PCA}
db_kmeans_pca_df <- index.DB(dados_pca_df, kmeans_res_pca_df$cluster)$DB
db_pam_pca_df <- index.DB(dados_pca_df, pam_res_pca_df$clustering)$DB
db_hc_pca_df <- index.DB(dados_pca_df, hc_clusters_pca_df)$DB
```

### RESULTADOS
```{r Resultados após PCA}
cat("Resultados (Métricas Internas):\n")
cat("K-Means - Silhueta:", mean(sil_kmeans_pca_df[, 3]),
    " WSS:", wss_kmeans_pca_df,
    " DBI:", db_kmeans_pca_df, "\n")

cat("PAM     - Silhueta:", mean(sil_pam_pca_df[, 3]),
    " WSS:", wss_pam_pca_df,
    " DBI:", db_pam_pca_df, "\n")

cat("Hierárq - Silhueta:", mean(sil_hc_pca_df[, 3]),
    " WSS:", wss_hc_pca_df,
    " DBI:", db_hc_pca_df, "\n")

resultados_pca_df <- data.frame(
  Algoritmo = c("K-Means", "PAM", "Hierárquico"),
  Silhueta = c(mean(sil_kmeans_pca_df[, 3]), mean(sil_pam_pca_df[, 3]), mean(sil_hc_pca_df[, 3])),
  WSS = c(wss_kmeans_pca_df, wss_pam_pca_df, wss_hc_pca_df),
  DBI = c(db_kmeans_pca_df, db_pam, db_hc_pca_df)
)

print(resultados_pca_df)
```

### Criação do Gráfico com os resultados após o PCA
```{r Gráfico Silhueta após o PCA}
p4 <- ggplot(resultados_pca_df, aes(x = Algoritmo, y = Silhueta, fill = Algoritmo)) +
  geom_bar(stat = "identity") +
  ggtitle("Comparação - Coeficiente de Silhueta") +
  theme_minimal()
```

```{r Gráfico WSS após o PCA}
p5 <- ggplot(resultados, aes(x = Algoritmo, y = WSS, fill = Algoritmo)) +
  geom_bar(stat = "identity") +
  ggtitle("Comparação - WSS (Within-cluster Sum of Squares)") +
  theme_minimal()
```

```{r Gráfico DBI após o PCA}
p6 <- ggplot(resultados, aes(x = Algoritmo, y = DBI, fill = Algoritmo)) +
  geom_bar(stat = "identity") +
  ggtitle("Comparação - Davies-Bouldin Index") +
  theme_minimal()
```

```{r Organizar em grid 1x3 após PCA}
grid.arrange(p4, p5, p6, nrow = 3)
```

Após a aplicação do PCA, que reduziu a dimensionalidade dos dados mantendo a maior parte da variância explicada, observou-se uma melhora significativa nos resultados dos modelos.
Com a nova representação dos dados, o algoritmo K-Means passou a liderar em todas as métricas, apresentando maior coesão interna (menor WSS), melhor separação entre os clusters (maior silhueta) e menor sobreposição (menor DBI). Isso evidencia o impacto positivo da redução de dimensionalidade na performance dos modelos de agrupamento.

## Tableau

Segue o link do gráfico solicitado na sexta questão:
[Gráfico salvo no Tableau Server](https://public.tableau.com/app/profile/marcos.perazo.viana/viz/Livro1_17590972068770/Grficodeespalhamento?publish=yes) e o print do ambiente Tableau:
![Ambiente RStudio](C:\Users\peraz\OneDrive\Arquivo\Marcos\Curso_de_Data_Science\Segmentacao\Visualizacao_e_Relatorios_de_Segmento\Projeto\imagens\Tableau.png)
